import aigco
from transformers import AutoTokenizer
from huggingface_hub import snapshot_download
from dotenv import load_dotenv

load_dotenv()

# æ¨¡å‹åœ¨ HF ä¸Šçš„ ID
REPO_ID = "Qwen/Qwen3-0.6B"

# ç¡®ä¿ logger é…ç½®äº†æ–‡ä»¶è¾“å‡º
# å¦‚æœ aigco.logger æ”¯æŒï¼Œå¯ä»¥ç›´æ¥åœ¨è¿™é‡ŒæŒ‡å®š filename
logger = aigco.logger(name="qwen3_inference")


def main():
    # è‡ªåŠ¨è·å–ç¼“å­˜ä¸­çš„çœŸå®ç»å¯¹è·¯å¾„
    try:
        # local_files_only=True ç¡®ä¿å®ƒåªä»æœ¬åœ°æ‰¾ï¼Œä¸ä¼šå»è”ç½‘ä¸‹è½½
        model_path = snapshot_download(repo_id=REPO_ID, local_files_only=True)
        logger.info(f"ğŸ“ æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {model_path}")
    except Exception as e:
        logger.error(f"âŒ æ— æ³•åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°æ¨¡å‹ {REPO_ID}ï¼ŒåŸå› : {e}")
        return

    # ä½¿ç”¨è‡ªåŠ¨è·å–çš„è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # åˆå§‹åŒ– LLM
    logger.info("å¼€å§‹åˆå§‹åŒ– LLM å¼•æ“...")
    llm = aigco.inference.LLM(model_path, enforce_eager=True, tensor_parallel_size=1)

    sampling_params = aigco.inference.SamplingParams(temperature=0.6, max_tokens=256)
    prompts_text = ["introduce yourself", "list all prime numbers within 100"]

    prompts = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}],
            tokenize=False,
            add_generation_prompt=True,
        )
        for prompt in prompts_text
    ]

    # æ‰§è¡Œç”Ÿæˆ
    logger.info(f"æ­£åœ¨ç”Ÿæˆå“åº”ï¼Œæ ·æœ¬æ•°é‡: {len(prompts)}...")
    outputs = llm.generate(prompts, sampling_params)

    # éå†å¹¶è®°å½•ç»“æœ
    for prompt, output in zip(prompts, outputs):
        log_message = f"\nPrompt: {prompt!r}\nCompletion: {output['text']!r}"

        # åŒæ—¶åœ¨æ§åˆ¶å°æ‰“å°å’Œè®°å…¥æ—¥å¿—æ–‡ä»¶
        # print(log_message)
        logger.info(log_message)

    logger.info("æ¨ç†ä»»åŠ¡å®Œæˆã€‚")


if __name__ == "__main__":
    main()
